{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "libs",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ed0001f3-6291-4a1d-bab2-da39e907cacb",
   "metadata": {
    "name": "intro",
    "collapsed": false,
    "resultHeight": 442
   },
   "source": "RAG Made Easy w/ Snowflake Cortex\n========\n\nCreating an end-to-end Retrieval Augmented Generation process (or RAG) directly in Snowflake.\n1) Extract full text from PDF files using Cortex and PARSE_DOC.\n2) Chunk those documents using SPLIT_TEXT.\n3) Use Cortex Search build a custom search service for your store of knowledge.\n4) Use foundational models to create a tailored answer.\n5) Test with a mini Streamlit app\n\nLibraries / packages used:\n- ~~PyPDF2 : reading of PDF files~~ [Replaced with PARSE_DOCUMENT]\n- ~~Langchain : for chunking~~ [Replaced with SPLIT_TEXT]\n- Snowpark ML Python : Python API for Snowflake\n- Snowflake Core : Rest API for Snowflake Service"
  },
  {
   "cell_type": "code",
   "id": "9d86cf9c-f0d1-4834-b143-cb39517071c3",
   "metadata": {
    "language": "sql",
    "name": "show_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 438
   },
   "outputs": [],
   "source": "-- Optional set up: Place your MD files in a stage for extraction\nls @huberman;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ea74c09-b8d0-440d-84d1-fd1cfaef9ce3",
   "metadata": {
    "language": "sql",
    "name": "raw_table",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE NEW_RAW_TEXT AS\nSELECT\n    relative_path as episode_name\n    , file_url\n    , TO_VARCHAR(\n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n        '@LLM_DEMO.PODCASTS.HUBERMAN',\n        relative_path,\n        {'mode': 'LAYOUT'}):content\n    ) AS raw_text\nfrom directory(@huberman);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32edb09d-b384-4e05-bb81-d730f425fde0",
   "metadata": {
    "language": "sql",
    "name": "only_for_me",
    "resultHeight": 112,
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "UPDATE\n  NEW_RAW_TEXT\nSET\n  raw_text = REPLACE (raw_text, 'Transcribed by readthatpodcast.com', '');\n\n\nSELECT * FROM NEW_RAW_TEXT LIMIT 1;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e633b10e-8255-4e67-ad98-0d2704af274a",
   "metadata": {
    "language": "sql",
    "name": "all_the_things",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "--Optional : This no longer fails due to exceeding token limits, but chunking is still important!\nSELECT\nSNOWFLAKE.CORTEX.SUMMARIZE(raw_text)\nFROM\nNEW_RAW_TEXT\nLIMIT 1;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0867fd7-eb9a-498c-bd08-ace4c763a965",
   "metadata": {
    "language": "sql",
    "name": "all_the_tokens",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS('summarize' , raw_text ) FROM\nNEW_RAW_TEXT\nLIMIT 1;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b06bb86-7434-4b11-aee4-41af529095c7",
   "metadata": {
    "name": "word_on_chunking",
    "collapsed": false,
    "resultHeight": 220
   },
   "source": "A note on chunking\n-----\nChunking is the process of splitting a large body of text into smaller 'chunks' whilst attempting to keep as much relevant information as possible. Make the chunks too small and you run the risk of removing key information that the model requires to answer the question. Too large and it may be harder to retreive the correct body of text from the vector search - or spend tokens excessively.\n\nThere are many strategies towards chunking. Eg - pass the most relevant, top n relevant chunks, or pass the most relevent chunk + the chunk either side of that one. Play around and see what works for your use case!\n"
  },
  {
   "cell_type": "code",
   "id": "6ef44130-b787-495d-9720-af0e584decc3",
   "metadata": {
    "language": "sql",
    "name": "chunked_table",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "--Create the chunked version of the table\nCREATE OR REPLACE TABLE NEW_CHUNK_TEXT AS\nSELECT\n        episode_name,\n        TO_VARCHAR(c.value) as chunk\n    FROM new_raw_text,\n   LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n      raw_text,\n      'none',\n      4000, --how many characters per row\n      0 --how much overlap should there be?\n   )) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68f422cd-8093-46ae-8499-81f0db590bf5",
   "metadata": {
    "language": "sql",
    "name": "cell4",
    "collapsed": true,
    "resultHeight": 112,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "UPDATE\n  NEW_CHUNK_TEXT\nSET\n  chunk = REPLACE (chunk, 'Transcribed by readthatpodcast.com', '');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "908efcfe-edad-460d-901c-cf464f7df4a2",
   "metadata": {
    "language": "sql",
    "name": "chunked_text",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM NEW_CHUNK_TEXT LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3dd8348b-f030-4f5e-81e1-cc02497d1497",
   "metadata": {
    "language": "sql",
    "name": "Chunks_per_ep",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT EPISODE_NAME, COUNT(*) FROM NEW_CHUNK_TEXT GROUP BY 1;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b3265c0-5506-4fd1-af10-6f5777a8d33c",
   "metadata": {
    "language": "sql",
    "name": "Create_Search",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE NEW_HUBERMAN\n  ON CHUNK\n  ATTRIBUTES EPISODE_NAME\n  WAREHOUSE = tc_wh\n  TARGET_LAG = '7 days'\n  AS (\n    (\n    SELECT\n        CHUNK,\n        EPISODE_NAME\n    FROM NEW_CHUNK_TEXT\n)\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d7df276-d170-4281-b6b5-a1680ee900a2",
   "metadata": {
    "language": "sql",
    "name": "SQL_Search",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "--query it with SQL\nSELECT PARSE_JSON(\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n      'NEW_HUBERMAN',\n      '{\n         \"query\": \"How do I make time go slower?\",\n         \"columns\":[\n            \"CHUNK\",\n            \"EPISODE_NAME\"\n         ],\n         \"limit\":3\n      }'\n  )\n)['results'] as results;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1454493-84ef-430d-96e8-c67482b06897",
   "metadata": {
    "language": "python",
    "name": "Python_Search",
    "collapsed": false,
    "resultHeight": 1577
   },
   "outputs": [],
   "source": "#query it with Python\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\nimport streamlit as st\nimport json\nimport pandas as pd\n\nsession = get_active_session()\n\nprompt=\"How do I make time go slower?\"\n\nroot = Root(session)\n\n# query service\nsvc = (root\n  .databases[\"LLM_DEMO\"]\n  .schemas[\"PODCASTS\"]\n  .cortex_search_services[\"NEW_HUBERMAN\"]\n)\n\nresp = svc.search(\n  query=prompt,\n  columns=[\"CHUNK\", \"EPISODE_NAME\"],\n  limit=3\n).to_json()\n\n#optional - I just like the way this looks...\njson_conv = json.loads(resp) if isinstance(resp, str) else resp\nsearch_df = pd.json_normalize(json_conv['results'])\n\n#st.write(search_df)\nfor _, row in search_df.iterrows():\n    st.write(f\"**{row['EPISODE_NAME']}**\")\n    st.caption(row['CHUNK'])\n    st.write('---')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abdd6751-1ff1-4932-9e9a-a42fbfd4afb7",
   "metadata": {
    "language": "sql",
    "name": "rag_in_a_sql_call",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "--Pass the chunk we need along with the prompt to get a better structured answer from the LLM  -- all in SQL!\nSELECT snowflake.cortex.complete(\n    'llama3.1-8b', \n    CONCAT( \n        'Answer the question based on the context. Context: ',\n        (\n            SELECT PARSE_JSON(\n      SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n          'NEW_HUBERMAN',\n          '{\n             \"query\": \"How do I make time go slower?\",\n             \"columns\":[\n                \"CHUNK\",\n                \"EPISODE_NAME\"\n             ],\n             \"limit\":3\n              }'\n          )\n)['results'] as results\n        ),\n        'Question: ', \n        'How do I make time go slower?',\n        'Answer: '\n    )\n) as response;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad8b0da2-aed2-4173-a8c6-13633e38f4d0",
   "metadata": {
    "language": "python",
    "name": "ask_your_data_app",
    "collapsed": false,
    "resultHeight": 1382
   },
   "outputs": [],
   "source": "from snowflake.cortex import Complete\n\nst.title(\"Ask Your Data Anything :snowflake:\")\nst.write(\"\"\"Built using end-to-end RAG in Snowflake with Cortex functions.\"\"\")\n\nmodel = st.selectbox('Select your model:',('mistral-large2','mistral-7b','llama3.1-8b','llama3.1-70b'))\n\nprompt = st.text_input(\"Enter prompt\", placeholder=\"What makes time perceived to be slower?\", label_visibility=\"collapsed\") \n\nresp = svc.search(\n  query=prompt,\n  columns=[\"CHUNK\", \"EPISODE_NAME\"],\n  limit=7\n).to_json()\n\nsystem_p = f'''\nAnswer the question ONLY using the context provided. Here is the context to use: \n### Context: \n{resp}'\n### Question:\n{prompt}\n'''\n\nif prompt:\n    LLM = Complete(model,system_p)\n    st.write(LLM)",
   "execution_count": null
  }
 ]
}